{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment imports several libraries for data manipulation, and visualization. Additionally, it sets some default parameters for matplotlib figures, such as the figure size, font size, and font weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [8,5]\n",
    "plt.rcParams['font.size'] =14\n",
    "plt.rcParams['font.weight']= 'bold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment reads a CSV file named 'insurance.csv' into a pandas DataFrame called 'df' using the `pd.read_csv()` function.\n",
    "It then prints the number of rows and columns in the dataset using the `shape` attribute of the DataFrame.\n",
    "Finally, it displays the first few rows of the DataFrame using the `head()` method, providing a glimpse of the dataset's structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('insurance.csv')\n",
    "print('\\nNumber of rows and columns in the data set: ', df.shape)\n",
    "print('')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates a scatter plot with a linear regression line fitted using seaborn's `lmplot()` function.\n",
    "It specifies the 'bmi' column from the DataFrame as the independent variable on the x-axis and the 'charges' column as the dependent variable on the y-axis.\n",
    "The `aspect` and `height` parameters control the aspect ratio and height of the plot, respectively.\n",
    "The plot is then labeled with appropriate x-axis and y-axis labels, and a title is added to describe the relationship between insurance charges and body mass index (BMI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.lmplot(x='bmi',y='charges',data=df,aspect=2,height=6)\n",
    "plt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')\n",
    "plt.ylabel('Insurance Charges: as Dependent variable')\n",
    "plt.title('Charge Vs BMI');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates a heatmap using seaborn's `heatmap()` function to visualize missing values in the DataFrame 'df'.\n",
    "It sets the figure size to (12, 4) and specifies the colormap 'viridis' for better visualization.\n",
    "The `cbar=False` argument removes the color bar, and `yticklabels=False` removes the y-axis tick labels for better clarity.\n",
    "The heatmap represents missing values as colored cells, where yellow indicates missing data and purple indicates existing data.\n",
    "The title 'Missing value in the dataset' provides context for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.heatmap(df.isnull(),cbar=False,cmap='viridis',yticklabels=False)\n",
    "plt.title('Missing value in the dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates a correlation plot using seaborn's `heatmap()` function to visualize the correlation matrix of selected columns in the DataFrame 'df'.\n",
    "The selected columns include 'age', 'bmi', 'children', and 'charges'.\n",
    "A new DataFrame 'new_df' is created as a copy of the selected data.\n",
    "The correlation matrix is calculated using the `corr()` method, and then plotted as a heatmap with 'Wistia' colormap.\n",
    "The heatmap displays correlation coefficients between pairs of variables, with annotations showing the exact correlation values for better interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation plot\n",
    "selected_data = df[['age', 'bmi', 'children', 'charges']]\n",
    "new_df = selected_data.copy()\n",
    "corr = new_df.corr()\n",
    "sns.heatmap(corr, cmap = 'Wistia', annot= True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates a figure with two subplots using matplotlib.\n",
    "In the first subplot (left), a distribution plot is generated for the 'charges' column from the DataFrame 'df' using seaborn's `distplot()` function.\n",
    "The data is divided into 50 bins, and the plot is colored red.\n",
    "The title 'Distribution of insurance charges' is added to describe the plot.\n",
    "In the second subplot (right), another distribution plot is created, this time for the logarithm (base 10) of the 'charges' column.\n",
    "The data is divided into 40 bins, and the plot is colored blue.\n",
    "The title 'Distribution of insurance charges in log scale' is added to describe the plot.\n",
    "Additionally, the x-axis scale is set to logarithmic using `ax.set_xscale('log')` to visualize the distribution on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= plt.figure(figsize=(12,4))\n",
    "\n",
    "ax=f.add_subplot(121)\n",
    "sns.distplot(df['charges'],bins=50,color='r',ax=ax)\n",
    "ax.set_title('Distribution of insurance charges')\n",
    "\n",
    "ax=f.add_subplot(122)\n",
    "sns.distplot(np.log10(df['charges']),bins=40,color='b',ax=ax)\n",
    "ax.set_title('Distribution of insurance charges in $log$ sacle')\n",
    "ax.set_xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates a figure with two subplots using matplotlib.\n",
    "In the first subplot (left), a violin plot is generated to visualize the distribution of insurance charges ('charges') across different values of the 'sex' variable.\n",
    "Seaborn's `violinplot()` function is used for this purpose.\n",
    "The plot is colored using the 'Wistia' palette, and the title 'Violin plot of Charges vs sex' is added to describe the plot.\n",
    "In the second subplot (right), another violin plot is created to visualize the distribution of charges across different values of the 'smoker' variable. The plot is colored using the 'magma' palette, and the title 'Violin plot of Charges vs smoker' is added.\n",
    "Each violin plot displays the distribution of charges, with the width of the violin indicating the density of data points at different charge levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(14,6))\n",
    "ax = f.add_subplot(121)\n",
    "sns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\n",
    "ax.set_title('Violin plot of Charges vs sex')\n",
    "\n",
    "ax = f.add_subplot(122)\n",
    "sns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\n",
    "ax.set_title('Violin plot of Charges vs smoker');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates a box plot to visualize the distribution of insurance charges ('charges') across different numbers of children ('children'), with the additional differentiation based on the 'sex' variable.\n",
    "Seaborn's `boxplot()` function is utilized for this purpose.\n",
    "The plot is colored using the 'rainbow' palette to differentiate between different levels of the 'sex' variable.\n",
    "The title 'Box plot of charges vs children' provides context for the plot, indicating the relationship being examined.\n",
    "Each box represents the interquartile range (IQR) of charges for a specific number of children, with the median indicated by the horizontal line inside the box.\n",
    "The whiskers extend to the minimum and maximum values within 1.5 times the IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "sns.boxplot(x='children', y='charges',hue='sex',data=df,palette='rainbow')\n",
    "plt.title('Box plot of charges vs children');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment groups the DataFrame 'new_df' by the 'children' column and then aggregates the 'charges' column using the mean, minimum, and maximum functions.\n",
    "Specifically, it calculates the mean, minimum, and maximum values of insurance charges for each unique value of the 'children' variable.\n",
    "The result is a DataFrame showing the mean, minimum, and maximum charges for each category of the number of children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.groupby('children').agg(['mean','min','max'])['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates a violin plot to visualize the distribution of insurance charges ('charges') across different regions ('region'), with the additional differentiation based on the 'sex' variable. Seaborn's `violinplot()` function is utilized for this purpose. The plot is colored using the 'rainbow' palette, and the `split=True` parameter is used to split the violins by the 'sex' variable. The title 'Violin plot of charges vs children' provides context for the plot, although it seems to be incorrectly labeled, as it should indicate the relationship between charges and regions. Each violin represents the distribution of charges within a specific region, and the split violins show the distribution based on gender within each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "sns.violinplot(x='region', y='charges',hue='sex',data=df,palette='rainbow',split=True)\n",
    "plt.title('Violin plot of charges vs children');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates a figure with two subplots using Matplotlib. In the first subplot (`ax = f.add_subplot(121)`), it creates a scatter plot of 'charges' against 'age' from the DataFrame `df`, distinguishing data points by 'smoker' status using different colors. The title of this subplot is set to 'Scatter plot of Charges vs age'. In the second subplot (`ax = f.add_subplot(122)`), it creates a scatter plot of 'charges' against 'bmi' from the DataFrame `df`, again distinguishing data points by 'smoker' status. The title of this subplot is set to 'Scatter plot of Charges vs bmi'. The `plt.savefig('sc.png')` command saves the entire figure as a PNG file named 'sc.png'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(14,6))\n",
    "ax = f.add_subplot(121)\n",
    "sns.scatterplot(x='age',y='charges',data=df,palette='magma',hue='smoker',ax=ax)\n",
    "ax.set_title('Scatter plot of Charges vs age')\n",
    "\n",
    "ax = f.add_subplot(122)\n",
    "sns.scatterplot(x='bmi',y='charges',data=df,palette='viridis',hue='smoker')\n",
    "ax.set_title('Scatter plot of Charges vs bmi')\n",
    "plt.savefig('sc.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment utilizes the Pandas library to perform one-hot encoding on categorical variables within a DataFrame named `df`. The `categorical_columns` list holds the names of the categorical columns. By employing the `pd.get_dummies()` function, the categorical variables are transformed into binary dummy variables. Parameters such as `prefix` and `prefix_sep` allow customization of the naming convention for the newly created dummy columns. The `drop_first` parameter is set to True to prevent multicollinearity in regression models by dropping the first level of each categorical variable. Finally, the resulting DataFrame `df_encode` contains both the original columns and the newly generated dummy variables, facilitating further analysis or modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy variable\n",
    "categorical_columns = ['sex','children', 'smoker', 'region']\n",
    "df_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_', columns = categorical_columns, drop_first =True, dtype='int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment is designed to verify the one-hot encoding process by printing out information about the original DataFrame `df` and the encoded DataFrame `df_encode`. The `print()` statements provide insights into the structure and dimensions of the two DataFrames. The first `print()` statement displays the column names of the original DataFrame `df`. The second `print()` statement shows the number of rows and columns in the original DataFrame. The third `print()` statement presents the column names of the encoded DataFrame `df_encode`. Finally, the fourth `print()` statement reveals the number of rows and columns in the encoded DataFrame. By comparing these outputs, one can verify the transformation process and ensure the integrity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets verify the dummay variable process\n",
    "print('Columns in original data frame:\\n',df.columns.values)\n",
    "print('\\nNumber of rows and columns in the dataset:',df.shape)\n",
    "print('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\n",
    "print('\\nNumber of rows and columns in the dataset:',df_encode.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment utilizes the `boxcox` function from the `scipy.stats` module to perform a Box-Cox transformation on the 'charges' column of the DataFrame `df_encode`. The Box-Cox transformation aims to stabilize variance and make the data more normally distributed. The function returns three values: `y_bc`, which is the transformed data, `lam`, which is the lambda parameter of the transformation, and `ci`, which is the confidence interval of the lambda parameter. However, it seems that the Box-Cox transformation did not yield better results for the model, so it's commented out and instead, a log transformation is employed for the 'charges' column. The confidence interval (`ci`) and the lambda parameter (`lam`) are printed to assess the significance and effectiveness of the transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "y_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n",
    "ci,lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment applies a logarithmic transformation to the 'charges' column of the DataFrame `df_encode`. The `np.log()` function from the NumPy library is used to compute the natural logarithm of each value in the 'charges' column. Logarithmic transformations are often employed to address skewness in data and to make it more symmetrically distributed, which can be beneficial for certain modeling techniques. By transforming the 'charges' column in this manner, it prepares the data for analysis or modeling tasks where assumptions of normality or linear relationships are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log transform\n",
    "df_encode['charges'] = np.log(df_encode['charges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet employs the train_test_split function from the sklearn.model_selection module to partition the dataset into training and testing sets for both independent and dependent variables. Initially, it constructs the independent variable matrix X by dropping the 'charges' column from the DataFrame df_encode, assigning the dependent variable y to the 'charges' column. Subsequently, it applies the train_test_split function to split X and y into training and testing sets, allocating 70% of the data for training (X_train and y_train) and 30% for testing (X_test and y_test). The random_state parameter ensures reproducibility by fixing the random seed for the split, thereby maintaining consistency across multiple runs. This division enables subsequent model training on the training set and evaluation on the independent testing set, facilitating the assessment of model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_encode.drop('charges',axis=1) # Independet variable\n",
    "y = df_encode['charges'] # dependent variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment represents the two steps involved in building a linear regression model:\n",
    "\n",
    "1. **Adding Intercept Term**: The first step involves adding a column of ones (representing \\(x_0 = 1\\)) to the independent variable matrices for both the training and testing datasets. This operation is performed using NumPy's `np.c_[]` function to concatenate a column vector of ones with the independent variable matrices `X_train` and `X_test`, resulting in `X_train_0` and `X_test_0`, respectively.\n",
    "\n",
    "2. **Building the Model**: In the second step, the parameters (\\(\\theta\\)) of the linear regression model are estimated. This is done by applying the normal equation, which involves matrix operations. Specifically, it calculates the parameter vector \\(\\theta\\) by multiplying the inverse of the matrix product of the transpose of `X_train_0` and `X_train_0` with the matrix product of the transpose of `X_train_0` and the target variable `y_train`. The resulting parameter vector `theta` represents the coefficients of the linear regression model.\n",
    "\n",
    "These steps are fundamental in preparing the data and building the linear regression model, which can then be used for making predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: add x0 =1 to dataset\n",
    "X_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
    "X_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n",
    "\n",
    "# Step2: build model\n",
    "theta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet constructs a DataFrame `parameter_df` to store the parameters of a linear regression model. Initially, it generates a list of parameter names labeled as 'theta_0', 'theta_1', ..., 'theta_n', where 'n' represents the number of features plus one (including the intercept term). Next, it creates a list of column names for the DataFrame, starting with the intercept term ('intersect:x_0=1') followed by the names of the independent variables. Finally, the DataFrame `parameter_df` is created with three columns: 'Parameter', 'Columns', and 'theta', containing the parameter names, corresponding column names, and parameter values, respectively. This organized structure provides a comprehensive overview of the linear regression model's parameters, facilitating further analysis and interpretation of its coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters for linear regression model\n",
    "parameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\n",
    "columns = ['intersect:x_0=1'] + list(X.columns.values)\n",
    "parameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet constructs a DataFrame `parameter_df` to store the parameters of a linear regression model. Initially, it generates a list of parameter names labeled as 'theta_0', 'theta_1', ..., 'theta_n', where 'n' represents the number of features plus one (including the intercept term). Next, it creates a list of column names for the DataFrame, starting with the intercept term ('intersect:x_0=1') followed by the names of the independent variables. Finally, the DataFrame `parameter_df` is created with three columns: 'Parameter', 'Columns', and 'theta', containing the parameter names, corresponding column names, and parameter values, respectively. This organized structure provides a comprehensive overview of the linear regression model's parameters, facilitating further analysis and interpretation of its coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n",
    "\n",
    "#Parameter\n",
    "sk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\n",
    "parameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\n",
    "parameter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment evaluates the performance of a linear regression model obtained using the normal equation method by computing two key metrics: Mean Squared Error (MSE) and R-squared. Firstly, it calculates the predicted values of the target variable using the test dataset and the parameter vector derived from the normal equation. Then, it computes the MSE by averaging the squared differences between the predicted and actual target values. Additionally, it determines the R-squared value, which quantifies the proportion of variance in the target variable that is explained by the independent variables. A higher R-squared value indicates a better fit of the model to the data. By printing out both the MSE and R-squared, this code provides insights into the accuracy and explanatory power of the linear regression model trained using the normal equation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal equation\n",
    "y_pred_norm =  np.matmul(X_test_0,theta)\n",
    "\n",
    "#Evaluvation: MSE\n",
    "J_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n",
    "\n",
    "# R_square \n",
    "sse = np.sum((y_pred_norm - y_test)**2)\n",
    "sst = np.sum((y_test - y_test.mean())**2)\n",
    "R_square = 1 - (sse/sst)\n",
    "print('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\n",
    "print('R square obtain for normal equation method is :',R_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code evaluates the performance of a linear regression model trained using the scikit-learn library. It first predicts the target variable values (`y_pred_sk`) using the test dataset `X_test` with the `predict()` method of the `LinearRegression` object `lin_reg`. Then, it calculates the Mean Squared Error (MSE) by comparing the predicted values `y_pred_sk` with the actual target values `y_test` using the `mean_squared_error()` function from the `sklearn.metrics` module. Additionally, it computes the R-squared value (`R_square_sk`) using the `score()` method of the `LinearRegression` object, which measures the proportion of variance in the target variable explained by the independent variables. Finally, the code prints out both the MSE and R-squared values, providing insights into the performance of the scikit-learn linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn regression module\n",
    "y_pred_sk = lin_reg.predict(X_test)\n",
    "\n",
    "#Evaluvation: MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "J_mse_sk = mean_squared_error(y_pred_sk, y_test)\n",
    "\n",
    "# R_square\n",
    "R_square_sk = lin_reg.score(X_test,y_test)\n",
    "print('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\n",
    "print('R square obtain for scikit learn library is :',R_square_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment visually assesses the linearity and residual normality of predictions made by a scikit-learn linear regression model. It creates a figure with two subplots: the first subplot displays a scatter plot of actual versus predicted values, allowing for the examination of linearity between the two; the second subplot presents a histogram of the residuals (the differences between actual and predicted values), enabling the evaluation of their normality and mean. The scatter plot in the first subplot helps determine whether the model's predictions exhibit a linear relationship with the actual values, while the histogram in the second subplot assesses the distribution of residual errors around the mean. These visualizations aid in understanding the model's performance and identifying potential issues such as non-linearity or non-normality in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Linearity\n",
    "f = plt.figure(figsize=(14, 5))\n",
    "ax1 = f.add_subplot(121)\n",
    "sns.scatterplot(x=y_test, y=y_pred_sk, ax=ax1, color='r')\n",
    "ax1.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n",
    "\n",
    "# Check for Residual normality & mean\n",
    "ax2 = f.add_subplot(122)\n",
    "sns.distplot((y_test - y_pred_sk), ax=ax2, color='b')\n",
    "ax2.axvline((y_test - y_pred_sk).mean(), color='k', linestyle='--')\n",
    "ax2.set_title('Check for Residual normality & mean: \\n Residual error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment examines multivariate normality and homoscedasticity of residuals from predictions made by the scikit-learn linear regression model. Firstly, it creates a figure with two subplots: the first subplot displays a Quantile-Quantile (Q-Q) plot, comparing the distribution of residuals against a theoretical normal distribution to assess multivariate normality. The Q-Q plot is generated using the `probplot` function from the `scipy.stats` module, with the residuals passed as input. The second subplot visualizes homoscedasticity, the equality of variance in residuals across different levels of predicted values, using a scatter plot of residuals against predicted values. This subplot helps identify any patterns or trends in the spread of residuals as predicted values change. These visual diagnostics aid in evaluating the assumptions of linear regression and understanding the behavior of residuals in relation to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Multivariate Normality\n",
    "# Quantile-Quantile plot \n",
    "f,ax = plt.subplots(1,2,figsize=(14,6))\n",
    "import scipy as sp\n",
    "_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\n",
    "ax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n",
    "\n",
    "#Check for Homoscedasticity\n",
    "sns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \n",
    "ax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variance Inflation Factor (VIF) is calculated as:\n",
    "\n",
    "VIF = 1 / (1 - R^2)\n",
    "\n",
    "where R^2 is the R-squared value obtained from the linear regression model. A high VIF indicates that the variance of the coefficient estimates is inflated due to multicollinearity, suggesting that some independent variables may be highly correlated with others. This computation of the VIF provides insights into the extent of multicollinearity in the dataset and helps identify potential issues that may affect the reliability of the regression model's coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Multicollinearity\n",
    "#Variance Inflation Factor\n",
    "VIF = 1/(1- R_square_sk)\n",
    "VIF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
